{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "B1sijZIxLXuv",
        "outputId": "566f1181-5934-49a1-dff7-dd4dacbef7c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [1/90], Loss: 2.4880\n",
            "Epoch [1/10], Step [11/90], Loss: 1.5531\n",
            "Epoch [1/10], Step [21/90], Loss: 1.4318\n",
            "Epoch [1/10], Step [31/90], Loss: 0.8227\n",
            "Epoch [1/10], Step [41/90], Loss: 0.7327\n",
            "Epoch [1/10], Step [51/90], Loss: 0.5536\n",
            "Epoch [1/10], Step [61/90], Loss: 0.4236\n",
            "Epoch [1/10], Step [71/90], Loss: 0.6056\n",
            "Epoch [1/10], Step [81/90], Loss: 0.5765\n",
            "Epoch [1/10] Training Loss: 0.8830, Training Acc: 0.7604\n",
            "Model saved to /content/drive/MyDrive/Labelling/saved_models/best_model_epoch_1.pth\n",
            "Epoch [2/10], Step [1/90], Loss: 0.3321\n",
            "Epoch [2/10], Step [11/90], Loss: 0.2377\n",
            "Epoch [2/10], Step [21/90], Loss: 0.3902\n",
            "Epoch [2/10], Step [31/90], Loss: 0.4377\n",
            "Epoch [2/10], Step [41/90], Loss: 0.2763\n",
            "Epoch [2/10], Step [51/90], Loss: 0.3619\n",
            "Epoch [2/10], Step [61/90], Loss: 0.2749\n",
            "Epoch [2/10], Step [71/90], Loss: 0.3369\n",
            "Epoch [2/10], Step [81/90], Loss: 0.3778\n",
            "Epoch [2/10] Training Loss: 0.3639, Training Acc: 0.8944\n",
            "Model saved to /content/drive/MyDrive/Labelling/saved_models/best_model_epoch_2.pth\n",
            "Epoch [3/10], Step [1/90], Loss: 0.1932\n",
            "Epoch [3/10], Step [11/90], Loss: 0.2042\n",
            "Epoch [3/10], Step [21/90], Loss: 0.2675\n",
            "Epoch [3/10], Step [31/90], Loss: 0.1610\n",
            "Epoch [3/10], Step [41/90], Loss: 0.1722\n",
            "Epoch [3/10], Step [51/90], Loss: 0.2704\n",
            "Epoch [3/10], Step [61/90], Loss: 0.1680\n",
            "Epoch [3/10], Step [71/90], Loss: 0.3552\n",
            "Epoch [3/10], Step [81/90], Loss: 0.2650\n",
            "Epoch [3/10] Training Loss: 0.2243, Training Acc: 0.9368\n",
            "Model saved to /content/drive/MyDrive/Labelling/saved_models/best_model_epoch_3.pth\n",
            "Epoch [4/10], Step [1/90], Loss: 0.1863\n",
            "Epoch [4/10], Step [11/90], Loss: 0.0538\n",
            "Epoch [4/10], Step [21/90], Loss: 0.2087\n",
            "Epoch [4/10], Step [31/90], Loss: 0.2621\n",
            "Epoch [4/10], Step [41/90], Loss: 0.1346\n",
            "Epoch [4/10], Step [51/90], Loss: 0.0826\n",
            "Epoch [4/10], Step [61/90], Loss: 0.2458\n",
            "Epoch [4/10], Step [71/90], Loss: 0.0436\n",
            "Epoch [4/10], Step [81/90], Loss: 0.0765\n",
            "Epoch [4/10] Training Loss: 0.1248, Training Acc: 0.9705\n",
            "Model saved to /content/drive/MyDrive/Labelling/saved_models/best_model_epoch_4.pth\n",
            "Epoch [5/10], Step [1/90], Loss: 0.0582\n",
            "Epoch [5/10], Step [11/90], Loss: 0.0600\n",
            "Epoch [5/10], Step [21/90], Loss: 0.0880\n",
            "Epoch [5/10], Step [31/90], Loss: 0.0556\n",
            "Epoch [5/10], Step [41/90], Loss: 0.0295\n",
            "Epoch [5/10], Step [51/90], Loss: 0.2662\n",
            "Epoch [5/10], Step [61/90], Loss: 0.0277\n",
            "Epoch [5/10], Step [71/90], Loss: 0.0578\n",
            "Epoch [5/10], Step [81/90], Loss: 0.0668\n",
            "Epoch [5/10] Training Loss: 0.0626, Training Acc: 0.9899\n",
            "Model saved to /content/drive/MyDrive/Labelling/saved_models/best_model_epoch_5.pth\n",
            "Epoch [6/10], Step [1/90], Loss: 0.0334\n",
            "Epoch [6/10], Step [11/90], Loss: 0.0208\n",
            "Epoch [6/10], Step [21/90], Loss: 0.0224\n",
            "Epoch [6/10], Step [31/90], Loss: 0.0082\n",
            "Epoch [6/10], Step [41/90], Loss: 0.0233\n",
            "Epoch [6/10], Step [51/90], Loss: 0.0535\n",
            "Epoch [6/10], Step [61/90], Loss: 0.0087\n",
            "Epoch [6/10], Step [71/90], Loss: 0.0064\n",
            "Epoch [6/10], Step [81/90], Loss: 0.0534\n",
            "Epoch [6/10] Training Loss: 0.0473, Training Acc: 0.9938\n",
            "Model saved to /content/drive/MyDrive/Labelling/saved_models/best_model_epoch_6.pth\n",
            "Epoch [7/10], Step [1/90], Loss: 0.0205\n",
            "Epoch [7/10], Step [11/90], Loss: 0.0155\n",
            "Epoch [7/10], Step [21/90], Loss: 0.0247\n",
            "Epoch [7/10], Step [31/90], Loss: 0.0068\n",
            "Epoch [7/10], Step [41/90], Loss: 0.0167\n",
            "Epoch [7/10], Step [51/90], Loss: 0.0094\n",
            "Epoch [7/10], Step [61/90], Loss: 0.0069\n",
            "Epoch [7/10], Step [71/90], Loss: 0.0077\n",
            "Epoch [7/10], Step [81/90], Loss: 0.0136\n",
            "Epoch [7/10] Training Loss: 0.0277, Training Acc: 0.9962\n",
            "Model saved to /content/drive/MyDrive/Labelling/saved_models/best_model_epoch_7.pth\n",
            "Epoch [8/10], Step [1/90], Loss: 0.0061\n",
            "Epoch [8/10], Step [11/90], Loss: 0.0058\n",
            "Epoch [8/10], Step [21/90], Loss: 0.0037\n",
            "Epoch [8/10], Step [31/90], Loss: 0.0027\n",
            "Epoch [8/10], Step [41/90], Loss: 0.0044\n",
            "Epoch [8/10], Step [51/90], Loss: 0.2726\n",
            "Epoch [8/10], Step [61/90], Loss: 0.0082\n",
            "Epoch [8/10], Step [71/90], Loss: 0.0117\n",
            "Epoch [8/10], Step [81/90], Loss: 0.0355\n",
            "Epoch [8/10] Training Loss: 0.0291, Training Acc: 0.9969\n",
            "Epoch [9/10], Step [1/90], Loss: 0.0153\n",
            "Epoch [9/10], Step [11/90], Loss: 0.0489\n",
            "Epoch [9/10], Step [21/90], Loss: 0.0072\n",
            "Epoch [9/10], Step [31/90], Loss: 0.0396\n",
            "Epoch [9/10], Step [41/90], Loss: 0.0979\n",
            "Epoch [9/10], Step [51/90], Loss: 0.0029\n",
            "Epoch [9/10], Step [61/90], Loss: 0.0065\n",
            "Epoch [9/10], Step [71/90], Loss: 0.0038\n",
            "Epoch [9/10], Step [81/90], Loss: 0.0304\n",
            "Epoch [9/10] Training Loss: 0.0205, Training Acc: 0.9962\n",
            "Model saved to /content/drive/MyDrive/Labelling/saved_models/best_model_epoch_9.pth\n",
            "Epoch [10/10], Step [1/90], Loss: 0.0025\n",
            "Epoch [10/10], Step [11/90], Loss: 0.0091\n",
            "Epoch [10/10], Step [21/90], Loss: 0.0063\n",
            "Epoch [10/10], Step [31/90], Loss: 0.0060\n",
            "Epoch [10/10], Step [41/90], Loss: 0.0041\n",
            "Epoch [10/10], Step [51/90], Loss: 0.0018\n",
            "Epoch [10/10], Step [61/90], Loss: 0.0055\n",
            "Epoch [10/10], Step [71/90], Loss: 0.0098\n",
            "Epoch [10/10], Step [81/90], Loss: 0.0055\n",
            "Epoch [10/10] Training Loss: 0.0165, Training Acc: 0.9979\n",
            "Model saved to /content/drive/MyDrive/Labelling/saved_models/best_model_epoch_10.pth\n",
            "Test Loss: 0.4079, Test Acc: 0.8941\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "from torchvision import transforms\n",
        "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Load datasets\n",
        "train_dataset_path = '/content/drive/MyDrive/Labelling/balanced_train_dataset.csv'\n",
        "test_dataset_path = '/content/drive/MyDrive/Labelling/balanced_test_dataset.csv'\n",
        "\n",
        "train_df = pd.read_csv(train_dataset_path)\n",
        "test_df = pd.read_csv(test_dataset_path)\n",
        "\n",
        "# Label encoding for sub_categories\n",
        "label_encoder = LabelEncoder()\n",
        "train_df['sub_category'] = label_encoder.fit_transform(train_df['sub_category'])\n",
        "test_df['sub_category'] = label_encoder.transform(test_df['sub_category'])\n",
        "\n",
        "# Custom dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, transform=None):\n",
        "        self.data = data\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_url = self.data.iloc[idx]['image']\n",
        "        label = self.data.iloc[idx]['sub_category']\n",
        "\n",
        "        response = requests.get(image_url)\n",
        "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Example transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Create datasets and DataLoaders with a limit of 90 batches\n",
        "train_dataset = CustomDataset(train_df, transform=transform)\n",
        "test_dataset = CustomDataset(test_df, transform=transform)\n",
        "\n",
        "# Subset to limit batches to 90 (32*90=2880 samples)\n",
        "train_subset_indices = np.random.choice(len(train_dataset), 2880, replace=False)\n",
        "train_loader = DataLoader(Subset(train_dataset, train_subset_indices), batch_size=32, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "# Define CNN model\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(32 * 56 * 56, 128)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.ReLU()(self.conv1(x))\n",
        "        x = nn.MaxPool2d(kernel_size=2)(x)\n",
        "        x = nn.ReLU()(self.conv2(x))\n",
        "        x = nn.MaxPool2d(kernel_size=2)(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = nn.ReLU()(self.fc1(x))\n",
        "        return x\n",
        "\n",
        "# Define hybrid model using vit_b_16 only\n",
        "class HybridModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(HybridModel, self).__init__()\n",
        "        self.cnn = SimpleCNN()\n",
        "\n",
        "        # Load vit_b_16 with weights\n",
        "        self.vit = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)\n",
        "\n",
        "        # Freeze ViT parameters to save memory, adjust if fine-tuning is needed\n",
        "        for param in self.vit.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        vit_dim = 1000  # Dimension of ViT model output\n",
        "        self.classifier = nn.Linear(128 + vit_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        cnn_out = self.cnn(x)\n",
        "\n",
        "        # Pass through ViT model and get the output tensor directly\n",
        "        vit_out = self.vit(x)  # Directly take the output tensor\n",
        "\n",
        "        # Concatenate outputs and pass to the classifier\n",
        "        combined = torch.cat((cnn_out, vit_out), dim=1)\n",
        "        return self.classifier(combined)\n",
        "\n",
        "# Initialize model, optimizer, and loss function\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = HybridModel(num_classes=len(label_encoder.classes_)).to(device)\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Directory for saving best model\n",
        "model_dir = '/content/drive/MyDrive/Labelling/saved_models'\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "\n",
        "# Training function with 90 batches per epoch\n",
        "def train_model(model, train_loader, optimizer, criterion, num_epochs=10):\n",
        "    best_val_loss = float('inf')\n",
        "    train_loss_history = []\n",
        "    train_acc_history = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total_batches = 90\n",
        "\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            if i >= total_batches:\n",
        "                break\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            if i % 10 == 0:\n",
        "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_batches}], Loss: {loss.item():.4f}')\n",
        "\n",
        "        train_loss = running_loss / total_batches\n",
        "        train_acc = correct / (total_batches * 32)\n",
        "        train_loss_history.append(train_loss)\n",
        "        train_acc_history.append(train_acc)\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}] Training Loss: {train_loss:.4f}, Training Acc: {train_acc:.4f}')\n",
        "\n",
        "        # Save best model based on training loss\n",
        "        if train_loss < best_val_loss:\n",
        "            best_val_loss = train_loss\n",
        "            torch.save(model.state_dict(), os.path.join(model_dir, f'best_model_epoch_{epoch+1}.pth'))\n",
        "            print(f'Model saved to {model_dir}/best_model_epoch_{epoch+1}.pth')\n",
        "\n",
        "    return train_loss_history, train_acc_history\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, test_loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    test_loss = running_loss / len(test_loader)\n",
        "    test_acc = correct / len(test_loader.dataset)\n",
        "    print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
        "    return test_loss, test_acc\n",
        "\n",
        "# Plotting function\n",
        "def plot_metrics(train_loss, train_acc):\n",
        "    epochs = range(1, len(train_loss) + 1)\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, train_loss, 'b-', label='Training Loss')\n",
        "    plt.title('Training Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, train_acc, 'b-', label='Training Accuracy')\n",
        "    plt.title('Training Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Train and evaluate the model\n",
        "train_loss_history, train_acc_history = train_model(model, train_loader, optimizer, criterion, num_epochs=10)\n",
        "test_loss, test_acc = evaluate_model(model, test_loader, criterion)\n",
        "plot_metrics(train_loss_history, train_acc_history)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install easyocr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgucxfTYRRYX",
        "outputId": "f3d61c49-258f-4b7c-8109-240ef51675f1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting easyocr\n",
            "  Downloading easyocr-1.7.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from easyocr) (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.10/dist-packages (from easyocr) (0.20.0+cu121)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from easyocr) (4.10.0.84)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from easyocr) (1.13.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from easyocr) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from easyocr) (10.4.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from easyocr) (0.24.0)\n",
            "Collecting python-bidi (from easyocr)\n",
            "  Downloading python_bidi-0.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from easyocr) (6.0.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from easyocr) (2.0.6)\n",
            "Collecting pyclipper (from easyocr)\n",
            "  Downloading pyclipper-1.3.0.post6-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting ninja (from easyocr)\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->easyocr) (1.3.0)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (2.35.1)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (2024.9.20)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (24.1)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->easyocr) (3.0.2)\n",
            "Downloading easyocr-1.7.2-py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyclipper-1.3.0.post6-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (912 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m912.2/912.2 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_bidi-0.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (286 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.8/286.8 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-bidi, pyclipper, ninja, easyocr\n",
            "Successfully installed easyocr-1.7.2 ninja-1.11.1.1 pyclipper-1.3.0.post6 python-bidi-0.6.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "import cv2\n",
        "import easyocr\n",
        "import random\n",
        "\n",
        "# Custom Hybrid Model Definition\n",
        "class CustomHybridModel(nn.Module):\n",
        "    def __init__(self, num_classes=11):\n",
        "        super(CustomHybridModel, self).__init__()\n",
        "        # CNN Branch\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # ViT Branch\n",
        "        self.vit = torch.hub.load('pytorch/vision:v0.13.0', 'vit_b_16', weights=\"IMAGENET1K_V1\")\n",
        "        self.vit.heads = nn.Identity()  # Removing classification head\n",
        "\n",
        "        # Fully Connected Layers for Hybrid Model\n",
        "        self.fc1 = nn.Linear(32 * 56 * 56 + 768, 256)  # Adjusted size after ViT & CNN branches\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # CNN Branch\n",
        "        cnn_out = self.pool(torch.relu(self.conv1(x)))\n",
        "        cnn_out = self.pool(torch.relu(self.conv2(cnn_out)))\n",
        "        cnn_out = cnn_out.view(-1, 32 * 56 * 56)\n",
        "\n",
        "        # ViT Branch\n",
        "        vit_out = self.vit(x)\n",
        "        if vit_out.dim() == 2:  # If output is 2D, it includes the [CLS] token\n",
        "            vit_out = vit_out[:, :768]  # Adjusting to ensure correct shape (batch size, hidden size)\n",
        "\n",
        "        # Combine CNN and ViT features\n",
        "        combined = torch.cat((cnn_out, vit_out), dim=1)\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        x = torch.relu(self.fc1(combined))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Load Model\n",
        "def load_model(model_path, num_classes=11):\n",
        "    model = CustomHybridModel(num_classes)\n",
        "    state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n",
        "    model.load_state_dict(state_dict, strict=False)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "# Label Mapping\n",
        "label_mapping = {\n",
        "    0: \"Backpack\",\n",
        "    1: \"Badminton Racquet\",\n",
        "    2: \"Kids' Watch\",\n",
        "    3: \"Refrigerator\",\n",
        "    4: \"Shirt\",\n",
        "    5: \"Speaker\",\n",
        "    6: \"Suitcase\",\n",
        "    7: \"Sunglasses\",\n",
        "    8: \"Television\",\n",
        "    9: \"Washing Machine\",\n",
        "    10: \"Men's Sports Shoe\"\n",
        "}\n",
        "\n",
        "# Color Extraction\n",
        "def extract_colors(image, n_colors=3):\n",
        "    img_np = np.array(image)\n",
        "    img_reshaped = img_np.reshape((-1, 3))\n",
        "    kmeans = KMeans(n_clusters=n_colors).fit(img_reshaped)\n",
        "    colors = kmeans.cluster_centers_.astype(int)\n",
        "\n",
        "    color_names = []\n",
        "    for color in colors:\n",
        "        r, g, b = color\n",
        "        if r > 200 and g > 200 and b > 200:\n",
        "            color_names.append(\"white\")\n",
        "        elif r > 200 and g < 100 and b < 100:\n",
        "            color_names.append(\"red\")\n",
        "        elif r < 100 and g > 200 and b < 100:\n",
        "            color_names.append(\"green\")\n",
        "        elif r < 100 and g < 100 and b > 200:\n",
        "            color_names.append(\"blue\")\n",
        "        elif r > 200 and g > 200 and b < 100:\n",
        "            color_names.append(\"yellow\")\n",
        "        elif r > 150 and g > 100 and b < 100:\n",
        "            color_names.append(\"orange\")\n",
        "        elif r < 50 and g < 50 and b < 50:\n",
        "            color_names.append(\"black\")\n",
        "        else:\n",
        "            color_names.append(\"gray\")\n",
        "\n",
        "    return list(set(color_names))\n",
        "\n",
        "# Shape Detection\n",
        "def detect_shapes(image):\n",
        "    gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)\n",
        "    edges = cv2.Canny(gray, 30, 150)\n",
        "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    shapes = []\n",
        "    for cnt in contours:\n",
        "        approx = cv2.approxPolyDP(cnt, 0.04 * cv2.arcLength(cnt, True), True)\n",
        "        if len(approx) == 3:\n",
        "            shapes.append(\"triangular\")\n",
        "        elif len(approx) == 4:\n",
        "            shapes.append(\"rectangular\")\n",
        "        elif len(approx) > 10:\n",
        "            shapes.append(\"circular\")\n",
        "        else:\n",
        "            shapes.append(\"irregular\")\n",
        "\n",
        "    return list(set(shapes))\n",
        "\n",
        "# Brand Extraction\n",
        "def extract_brand(image):\n",
        "    reader = easyocr.Reader(['en'])\n",
        "    ocr_results = reader.readtext(np.array(image), detail=0)\n",
        "\n",
        "    if ocr_results:\n",
        "        filtered_texts = [text for text in ocr_results if text.isalnum() and len(text) > 2]\n",
        "        if filtered_texts:\n",
        "            brand = max(set(filtered_texts), key=filtered_texts.count)\n",
        "            return brand.capitalize()\n",
        "    return \"Unbranded\"\n",
        "\n",
        "# Description Generation\n",
        "def generate_description(label, colors, shapes, brand):\n",
        "    color_desc = ', '.join(colors)\n",
        "    shape_desc = ', '.join(shapes)\n",
        "    accessory = \"\"\n",
        "\n",
        "    if \"Backpack\" in label:\n",
        "        accessory = \"with multiple compartments and durable straps\"\n",
        "    elif \"Suitcase\" in label:\n",
        "        accessory = \"featuring a telescopic handle and secure locks\"\n",
        "    elif \"Watch\" in label:\n",
        "        accessory = \"with a sleek wrist strap and water resistance\"\n",
        "    elif \"Shirt\" in label:\n",
        "        accessory = \"made from comfortable, breathable fabric\"\n",
        "\n",
        "    description_templates = [\n",
        "        f\"This is a {label.lower()} with a {color_desc} color scheme and a {shape_desc} design, {accessory}.\",\n",
        "        f\"A stylish {label.lower()} in shades of {color_desc}, {shape_desc} in structure, {accessory}.\",\n",
        "        f\"This {label.lower()} stands out with its {color_desc} tones and {shape_desc} shape, ideal for practical use. Brand: {brand}.\",\n",
        "        f\"An eye-catching {label.lower()} with {color_desc} colors, a {shape_desc} style, and {accessory}.\"\n",
        "    ]\n",
        "\n",
        "    if brand != \"Unbranded\":\n",
        "        description_templates.append(f\"A {label.lower()} by {brand}, showcasing {color_desc} hues, a {shape_desc} shape, and {accessory}.\")\n",
        "\n",
        "    return random.choice(description_templates)\n",
        "\n",
        "# Main Analysis Pipeline\n",
        "def analyze_image(image_path, model, label_mapping):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    img_tensor = transform(image).unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(img_tensor)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        label = label_mapping[predicted.item()]\n",
        "\n",
        "    colors = extract_colors(image)\n",
        "    shapes = detect_shapes(image)\n",
        "    brand = extract_brand(image)\n",
        "    description = generate_description(label, colors, shapes, brand)\n",
        "\n",
        "    print(f\"Label: {label}\")\n",
        "    print(f\"Colors: {colors}\")\n",
        "    print(f\"Shapes: {shapes}\")\n",
        "    print(f\"Brand: {brand}\")\n",
        "    print(f\"Description: {description}\")\n",
        "\n",
        "# Run Analysis\n",
        "model_path = '/content/drive/MyDrive/Labelling/saved_models/best_model_epoch_10.pth'\n",
        "model = load_model(model_path)\n",
        "\n",
        "# Analyze Image\n",
        "analyze_image('/content/123.webp', model, label_mapping)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vtdaE3ULrkf",
        "outputId": "34acaec0-9795-4a1f-9a7d-bd332e235323"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.13.0\n",
            "<ipython-input-9-5f5bfd5780f2>:50: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n",
            "WARNING:easyocr.easyocr:Downloading detection model, please wait. This may take several minutes depending upon your network connection.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Downloading recognition model, please wait. This may take several minutes depending upon your network connection.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: |██████████████████████████████████████████████████| 100.0% CompleteLabel: Television\n",
            "Colors: ['white', 'gray']\n",
            "Shapes: ['rectangular', 'triangular', 'irregular']\n",
            "Brand: Unbranded\n",
            "Description: An eye-catching television with white, gray colors, a rectangular, triangular, irregular style, and .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, Subset, random_split\n",
        "from torchvision import transforms\n",
        "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Load datasets\n",
        "train_dataset_path = '/content/drive/MyDrive/Labelling/balanced_train_dataset.csv'\n",
        "test_dataset_path = '/content/drive/MyDrive/Labelling/balanced_test_dataset.csv'\n",
        "\n",
        "train_df = pd.read_csv(train_dataset_path)\n",
        "test_df = pd.read_csv(test_dataset_path)\n",
        "\n",
        "# Label encoding for sub_categories\n",
        "label_encoder = LabelEncoder()\n",
        "train_df['sub_category'] = label_encoder.fit_transform(train_df['sub_category'])\n",
        "test_df['sub_category'] = label_encoder.transform(test_df['sub_category'])\n",
        "\n",
        "# Custom dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, transform=None):\n",
        "        self.data = data\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_url = self.data.iloc[idx]['image']\n",
        "        label = self.data.iloc[idx]['sub_category']\n",
        "\n",
        "        response = requests.get(image_url)\n",
        "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Example transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Create datasets and DataLoaders with a limit of 90 batches\n",
        "train_dataset = CustomDataset(train_df, transform=transform)\n",
        "test_dataset = CustomDataset(test_df, transform=transform)\n",
        "\n",
        "# Split train dataset into train and validation subsets\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_subset, batch_size=32, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "# Define CNN model\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(32 * 56 * 56, 128)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.ReLU()(self.conv1(x))\n",
        "        x = nn.MaxPool2d(kernel_size=2)(x)\n",
        "        x = nn.ReLU()(self.conv2(x))\n",
        "        x = nn.MaxPool2d(kernel_size=2)(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = nn.ReLU()(self.fc1(x))\n",
        "        return x\n",
        "\n",
        "# Define hybrid model using vit_b_16 only\n",
        "class HybridModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(HybridModel, self).__init__()\n",
        "        self.cnn = SimpleCNN()\n",
        "        self.vit = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)\n",
        "\n",
        "        # Freeze ViT parameters to save memory, adjust if fine-tuning is needed\n",
        "        for param in self.vit.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        vit_dim = 1000  # Dimension of ViT model output\n",
        "        self.classifier = nn.Linear(128 + vit_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        cnn_out = self.cnn(x)\n",
        "        vit_out = self.vit(x)\n",
        "        combined = torch.cat((cnn_out, vit_out), dim=1)\n",
        "        return self.classifier(combined)\n",
        "\n",
        "# Initialize model, optimizer, and loss function\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = HybridModel(num_classes=len(label_encoder.classes_)).to(device)\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model_dir = '/content/drive/MyDrive/Labelling/saved_models_2'\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "\n",
        "# Training function with validation step\n",
        "def train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs=10):\n",
        "    best_val_loss = float('inf')\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss, correct_train = 0.0, 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * images.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct_train += torch.sum(preds == labels)\n",
        "\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "        train_acc = correct_train.double() / len(train_loader.dataset)\n",
        "\n",
        "        # Validation step\n",
        "        model.eval()\n",
        "        val_loss, correct_val = 0.0, 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item() * images.size(0)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                correct_val += torch.sum(preds == labels)\n",
        "\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        val_acc = correct_val.double() / len(val_loader.dataset)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), os.path.join(model_dir, f'best_model_epoch_{epoch+1}.pth'))\n",
        "            print(f\"Model saved with Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Train and evaluate the model\n",
        "model = train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs=10)\n",
        "\n",
        "# Evaluation function for test data\n",
        "def evaluate_model(model, test_loader, criterion):\n",
        "    model.eval()\n",
        "    test_loss, correct = 0.0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item() * images.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == labels)\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_acc = correct.double() / len(test_loader.dataset)\n",
        "    print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
        "    return test_loss, test_acc\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_acc = evaluate_model(model, test_loader, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7g8lt-PxRO5M",
        "outputId": "c78acd10-05d1-46a9-acc8-645b22aa4b5c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Train Loss: 0.6403, Train Acc: 0.8151, Val Loss: 0.3840, Val Acc: 0.8920\n",
            "Model saved with Validation Loss: 0.3840\n",
            "Epoch 2/10, Train Loss: 0.2999, Train Acc: 0.9115, Val Loss: 0.3013, Val Acc: 0.9115\n",
            "Model saved with Validation Loss: 0.3013\n",
            "Epoch 3/10, Train Loss: 0.1809, Train Acc: 0.9467, Val Loss: 0.3219, Val Acc: 0.9038\n",
            "Epoch 4/10, Train Loss: 0.0909, Train Acc: 0.9780, Val Loss: 0.3388, Val Acc: 0.8976\n",
            "Epoch 5/10, Train Loss: 0.0407, Train Acc: 0.9918, Val Loss: 0.3254, Val Acc: 0.9045\n",
            "Epoch 6/10, Train Loss: 0.0258, Train Acc: 0.9956, Val Loss: 0.3469, Val Acc: 0.9073\n",
            "Epoch 7/10, Train Loss: 0.0230, Train Acc: 0.9962, Val Loss: 0.3657, Val Acc: 0.9052\n",
            "Epoch 8/10, Train Loss: 0.0156, Train Acc: 0.9979, Val Loss: 0.3812, Val Acc: 0.8976\n",
            "Epoch 9/10, Train Loss: 0.0135, Train Acc: 0.9974, Val Loss: 0.5210, Val Acc: 0.8746\n",
            "Epoch 10/10, Train Loss: 0.0157, Train Acc: 0.9974, Val Loss: 0.4837, Val Acc: 0.8948\n",
            "Test Loss: 0.4503, Test Acc: 0.8930\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "import cv2\n",
        "import easyocr\n",
        "import random\n",
        "\n",
        "# Custom Hybrid Model Definition\n",
        "class CustomHybridModel(nn.Module):\n",
        "    def __init__(self, num_classes=11):\n",
        "        super(CustomHybridModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.vit = torch.hub.load('pytorch/vision:v0.13.0', 'vit_b_16', weights=\"IMAGENET1K_V1\")\n",
        "        self.vit.heads = nn.Identity()  # Removing classification head\n",
        "\n",
        "        # Adjusted for compatibility\n",
        "        self.fc1 = nn.Linear(32 * 56 * 56 + 768, 128)  # Changed to match saved model\n",
        "        self.fc2 = nn.Linear(128, num_classes)  # Changed to match saved model\n",
        "\n",
        "    def forward(self, x):\n",
        "        cnn_out = self.pool(torch.relu(self.conv1(x)))\n",
        "        cnn_out = self.pool(torch.relu(self.conv2(cnn_out)))\n",
        "        cnn_out = cnn_out.view(-1, 32 * 56 * 56)\n",
        "\n",
        "        vit_out = self.vit(x)\n",
        "        if vit_out.dim() == 2:\n",
        "            vit_out = vit_out[:, :768]\n",
        "\n",
        "        combined = torch.cat((cnn_out, vit_out), dim=1)\n",
        "        x = torch.relu(self.fc1(combined))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Load Model\n",
        "def load_model(model_path, num_classes=11):\n",
        "    model = CustomHybridModel(num_classes)\n",
        "    state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n",
        "    model.load_state_dict(state_dict, strict=False)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "# Label Mapping\n",
        "label_mapping = {\n",
        "    0: \"Backpack\",\n",
        "    1: \"Badminton Racquet\",\n",
        "    2: \"Kids' Watch\",\n",
        "    3: \"Refrigerator\",\n",
        "    4: \"Shirt\",\n",
        "    5: \"Speaker\",\n",
        "    6: \"Suitcase\",\n",
        "    7: \"Sunglasses\",\n",
        "    8: \"Television\",\n",
        "    9: \"Washing Machine\",\n",
        "    10: \"Men's Sports Shoe\"\n",
        "}\n",
        "\n",
        "# Color Extraction\n",
        "def extract_colors(image, n_colors=3):\n",
        "    img_np = np.array(image)\n",
        "    img_reshaped = img_np.reshape((-1, 3))\n",
        "    kmeans = KMeans(n_clusters=n_colors).fit(img_reshaped)\n",
        "    colors = kmeans.cluster_centers_.astype(int)\n",
        "\n",
        "    color_names = []\n",
        "    for color in colors:\n",
        "        r, g, b = color\n",
        "        if r > 200 and g > 200 and b > 200:\n",
        "            color_names.append(\"white\")\n",
        "        elif r > 200 and g < 100 and b < 100:\n",
        "            color_names.append(\"red\")\n",
        "        elif r < 100 and g > 200 and b < 100:\n",
        "            color_names.append(\"green\")\n",
        "        elif r < 100 and g < 100 and b > 200:\n",
        "            color_names.append(\"blue\")\n",
        "        elif r > 200 and g > 200 and b < 100:\n",
        "            color_names.append(\"yellow\")\n",
        "        elif r > 150 and g > 100 and b < 100:\n",
        "            color_names.append(\"orange\")\n",
        "        elif r < 50 and g < 50 and b < 50:\n",
        "            color_names.append(\"black\")\n",
        "        else:\n",
        "            color_names.append(\"gray\")\n",
        "\n",
        "    return list(set(color_names))\n",
        "\n",
        "# Shape Detection\n",
        "def detect_shapes(image):\n",
        "    gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)\n",
        "    edges = cv2.Canny(gray, 30, 150)\n",
        "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    shapes = []\n",
        "    for cnt in contours:\n",
        "        approx = cv2.approxPolyDP(cnt, 0.04 * cv2.arcLength(cnt, True), True)\n",
        "        if len(approx) == 3:\n",
        "            shapes.append(\"triangular\")\n",
        "        elif len(approx) == 4:\n",
        "            shapes.append(\"rectangular\")\n",
        "        elif len(approx) > 10:\n",
        "            shapes.append(\"circular\")\n",
        "        else:\n",
        "            shapes.append(\"irregular\")\n",
        "\n",
        "    return list(set(shapes))\n",
        "\n",
        "# Brand Extraction\n",
        "def extract_brand(image):\n",
        "    reader = easyocr.Reader(['en'])\n",
        "    ocr_results = reader.readtext(np.array(image), detail=0)\n",
        "\n",
        "    if ocr_results:\n",
        "        filtered_texts = [text for text in ocr_results if text.isalnum() and len(text) > 2]\n",
        "        if filtered_texts:\n",
        "            brand = max(set(filtered_texts), key=filtered_texts.count)\n",
        "            return brand.capitalize()\n",
        "    return \"Unbranded\"\n",
        "\n",
        "# Description Generation\n",
        "def generate_description(label, colors, shapes, brand):\n",
        "    color_desc = ', '.join(colors)\n",
        "    shape_desc = ', '.join(shapes)\n",
        "    accessory = \"\"\n",
        "\n",
        "    if \"Backpack\" in label:\n",
        "        accessory = \"with multiple compartments and durable straps\"\n",
        "    elif \"Suitcase\" in label:\n",
        "        accessory = \"featuring a telescopic handle and secure locks\"\n",
        "    elif \"Watch\" in label:\n",
        "        accessory = \"with a sleek wrist strap and water resistance\"\n",
        "    elif \"Shirt\" in label:\n",
        "        accessory = \"made from comfortable, breathable fabric\"\n",
        "\n",
        "    description_templates = [\n",
        "        f\"This is a {label.lower()} with a {color_desc} color scheme and a {shape_desc} design, {accessory}.\",\n",
        "        f\"A stylish {label.lower()} in shades of {color_desc}, {shape_desc} in structure, {accessory}.\",\n",
        "        f\"This {label.lower()} stands out with its {color_desc} tones and {shape_desc} shape, ideal for practical use. Brand: {brand}.\",\n",
        "        f\"An eye-catching {label.lower()} with {color_desc} colors, a {shape_desc} style, and {accessory}.\"\n",
        "    ]\n",
        "\n",
        "    if brand != \"Unbranded\":\n",
        "        description_templates.append(f\"A {label.lower()} by {brand}, showcasing {color_desc} hues, a {shape_desc} shape, and {accessory}.\")\n",
        "\n",
        "    return random.choice(description_templates)\n",
        "\n",
        "# Main Analysis Pipeline\n",
        "def analyze_image(image_path, model, label_mapping):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    img_tensor = transform(image).unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(img_tensor)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        label = label_mapping[predicted.item()]\n",
        "\n",
        "    colors = extract_colors(image)\n",
        "    shapes = detect_shapes(image)\n",
        "    brand = extract_brand(image)\n",
        "    description = generate_description(label, colors, shapes, brand)\n",
        "\n",
        "    print(f\"Label: {label}\")\n",
        "    print(f\"Colors: {colors}\")\n",
        "    print(f\"Shapes: {shapes}\")\n",
        "    print(f\"Brand: {brand}\")\n",
        "    print(f\"Description: {description}\")\n",
        "\n",
        "# Run Analysis\n",
        "model_path = '/content/drive/MyDrive/Labelling/saved_models_2/best_model_epoch_1.pth'\n",
        "model = load_model(model_path)\n",
        "\n",
        "# Analyze Image\n",
        "analyze_image('/content/555.jpg', model, label_mapping)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjcCpa2ASnMC",
        "outputId": "d9fee693-074b-4d52-cb25-d554eeed2275"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.13.0\n",
            "<ipython-input-20-46506716ae73>:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: Backpack\n",
            "Colors: ['black', 'white', 'gray']\n",
            "Shapes: ['rectangular', 'triangular', 'irregular']\n",
            "Brand: Unbranded\n",
            "Description: This is a backpack with a black, white, gray color scheme and a rectangular, triangular, irregular design, with multiple compartments and durable straps.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "import cv2\n",
        "import easyocr\n",
        "import random\n",
        "\n",
        "# Custom Hybrid Model Definition\n",
        "class CustomHybridModel(nn.Module):\n",
        "    def __init__(self, num_classes=11):\n",
        "        super(CustomHybridModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.vit = torch.hub.load('pytorch/vision:v0.13.0', 'vit_b_16', weights=\"IMAGENET1K_V1\")\n",
        "        self.vit.heads = nn.Identity()  # Removing classification head\n",
        "\n",
        "        # Adjusted for compatibility\n",
        "        self.fc1 = nn.Linear(32 * 56 * 56 + 768, 128)  # Changed to match saved model\n",
        "        self.fc2 = nn.Linear(128, num_classes)  # Changed to match saved model\n",
        "\n",
        "    def forward(self, x):\n",
        "        cnn_out = self.pool(torch.relu(self.conv1(x)))\n",
        "        cnn_out = self.pool(torch.relu(self.conv2(cnn_out)))\n",
        "        cnn_out = cnn_out.view(-1, 32 * 56 * 56)\n",
        "\n",
        "        vit_out = self.vit(x)\n",
        "        if vit_out.dim() == 2:\n",
        "            vit_out = vit_out[:, :768]\n",
        "\n",
        "        combined = torch.cat((cnn_out, vit_out), dim=1)\n",
        "        x = torch.relu(self.fc1(combined))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Load Model\n",
        "def load_model(model_path, num_classes=11):\n",
        "    model = CustomHybridModel(num_classes)\n",
        "    state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n",
        "    model.load_state_dict(state_dict, strict=False)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "# Label Mapping\n",
        "label_mapping = {\n",
        "    0: \"Backpack\",\n",
        "    1: \"Badminton Racquet\",\n",
        "    2: \"Kids' Watch\",\n",
        "    3: \"Refrigerator\",\n",
        "    4: \"Shirt\",\n",
        "    5: \"Speaker\",\n",
        "    6: \"Suitcase\",\n",
        "    7: \"Sunglasses\",\n",
        "    8: \"Television\",\n",
        "    9: \"Washing Machine\",\n",
        "    10: \"Men's Sports Shoe\"\n",
        "}\n",
        "\n",
        "# Color Extraction\n",
        "def extract_colors(image, n_colors=3):\n",
        "    img_np = np.array(image)\n",
        "    img_reshaped = img_np.reshape((-1, 3))\n",
        "    kmeans = KMeans(n_clusters=n_colors).fit(img_reshaped)\n",
        "    colors = kmeans.cluster_centers_.astype(int)\n",
        "\n",
        "    color_names = []\n",
        "    for color in colors:\n",
        "        r, g, b = color\n",
        "        if r > 200 and g > 200 and b > 200:\n",
        "            color_names.append(\"white\")\n",
        "        elif r > 200 and g < 100 and b < 100:\n",
        "            color_names.append(\"red\")\n",
        "        elif r < 100 and g > 200 and b < 100:\n",
        "            color_names.append(\"green\")\n",
        "        elif r < 100 and g < 100 and b > 200:\n",
        "            color_names.append(\"blue\")\n",
        "        elif r > 200 and g > 200 and b < 100:\n",
        "            color_names.append(\"yellow\")\n",
        "        elif r > 150 and g > 100 and b < 100:\n",
        "            color_names.append(\"orange\")\n",
        "        elif r < 50 and g < 50 and b < 50:\n",
        "            color_names.append(\"black\")\n",
        "        else:\n",
        "            color_names.append(\"gray\")\n",
        "\n",
        "    return list(set(color_names))\n",
        "\n",
        "# Shape Detection\n",
        "def detect_shapes(image):\n",
        "    gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)\n",
        "    edges = cv2.Canny(gray, 30, 150)\n",
        "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    shapes = []\n",
        "    for cnt in contours:\n",
        "        approx = cv2.approxPolyDP(cnt, 0.04 * cv2.arcLength(cnt, True), True)\n",
        "        if len(approx) == 3:\n",
        "            shapes.append(\"triangular\")\n",
        "        elif len(approx) == 4:\n",
        "            shapes.append(\"rectangular\")\n",
        "        elif len(approx) > 10:\n",
        "            shapes.append(\"circular\")\n",
        "        else:\n",
        "            shapes.append(\"irregular\")\n",
        "\n",
        "    return list(set(shapes))\n",
        "\n",
        "# Brand Extraction\n",
        "def extract_brand(image):\n",
        "    reader = easyocr.Reader(['en'])\n",
        "    ocr_results = reader.readtext(np.array(image), detail=0)\n",
        "\n",
        "    if ocr_results:\n",
        "        filtered_texts = [text for text in ocr_results if text.isalnum() and len(text) > 2]\n",
        "        if filtered_texts:\n",
        "            brand = max(set(filtered_texts), key=filtered_texts.count)\n",
        "            return brand.capitalize()\n",
        "    return \"Unbranded\"\n",
        "\n",
        "# Description Generation\n",
        "def generate_description(label, colors, shapes, brand):\n",
        "    color_desc = ', '.join(colors)\n",
        "    shape_desc = ', '.join(shapes)\n",
        "    accessory = \"\"\n",
        "\n",
        "    if \"Backpack\" in label:\n",
        "        accessory = \"with multiple compartments and durable straps\"\n",
        "    elif \"Suitcase\" in label:\n",
        "        accessory = \"featuring a telescopic handle and secure locks\"\n",
        "    elif \"Watch\" in label:\n",
        "        accessory = \"with a sleek wrist strap and water resistance\"\n",
        "    elif \"Shirt\" in label:\n",
        "        accessory = \"made from comfortable, breathable fabric\"\n",
        "\n",
        "    description_templates = [\n",
        "        f\"This is a {label.lower()} with a {color_desc} color scheme and a {shape_desc} design, {accessory}.\",\n",
        "        f\"A stylish {label.lower()} in shades of {color_desc}, {shape_desc} in structure, {accessory}.\",\n",
        "        f\"This {label.lower()} stands out with its {color_desc} tones and {shape_desc} shape, ideal for practical use. Brand: {brand}.\",\n",
        "        f\"An eye-catching {label.lower()} with {color_desc} colors, a {shape_desc} style, and {accessory}.\"\n",
        "    ]\n",
        "\n",
        "    if brand != \"Unbranded\":\n",
        "        description_templates.append(f\"A {label.lower()} by {brand}, showcasing {color_desc} hues, a {shape_desc} shape, and {accessory}.\")\n",
        "\n",
        "    return random.choice(description_templates)\n",
        "\n",
        "# Main Analysis Pipeline\n",
        "def analyze_image(image_path, model, label_mapping):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    img_tensor = transform(image).unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(img_tensor)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        label = label_mapping[predicted.item()]\n",
        "\n",
        "    colors = extract_colors(image)\n",
        "    shapes = detect_shapes(image)\n",
        "    brand = extract_brand(image)\n",
        "    description = generate_description(label, colors, shapes, brand)\n",
        "\n",
        "    print(f\"Label: {label}\")\n",
        "    print(f\"Colors: {colors}\")\n",
        "    print(f\"Shapes: {shapes}\")\n",
        "    print(f\"Brand: {brand}\")\n",
        "    print(f\"Description: {description}\")\n",
        "\n",
        "# Run Analysis\n",
        "model_path = '/content/drive/MyDrive/Labelling/saved_models_2/best_model_epoch_2.pth'\n",
        "model = load_model(model_path)\n",
        "\n",
        "# Analyze Image\n",
        "analyze_image('/content/shirt.jpg', model, label_mapping)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPMxkXK4dBRr",
        "outputId": "f1baa21c-51fc-44b2-f761-9326811fa6b5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.13.0\n",
            "<ipython-input-4-203cddf90c3b>:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n",
            "WARNING:easyocr.easyocr:Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: Refrigerator\n",
            "Colors: ['white', 'gray']\n",
            "Shapes: ['triangular', 'irregular', 'rectangular']\n",
            "Brand: Unbranded\n",
            "Description: A stylish refrigerator in shades of white, gray, triangular, irregular, rectangular in structure, .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "import cv2\n",
        "import easyocr\n",
        "import random\n",
        "\n",
        "# Custom Hybrid Model Definition\n",
        "class CustomHybridModel(nn.Module):\n",
        "    def __init__(self, num_classes=11):\n",
        "        super(CustomHybridModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.vit = torch.hub.load('pytorch/vision:v0.13.0', 'vit_b_16', weights=\"IMAGENET1K_V1\")\n",
        "        self.vit.heads = nn.Identity()  # Removing classification head\n",
        "\n",
        "        # Adjusted for compatibility\n",
        "        self.fc1 = nn.Linear(32 * 56 * 56 + 768, 128)  # Changed to match saved model\n",
        "        self.fc2 = nn.Linear(128, num_classes)  # Changed to match saved model\n",
        "\n",
        "    def forward(self, x):\n",
        "        cnn_out = self.pool(torch.relu(self.conv1(x)))\n",
        "        cnn_out = self.pool(torch.relu(self.conv2(cnn_out)))\n",
        "        cnn_out = cnn_out.view(-1, 32 * 56 * 56)\n",
        "\n",
        "        vit_out = self.vit(x)\n",
        "        if vit_out.dim() == 2:\n",
        "            vit_out = vit_out[:, :768]\n",
        "\n",
        "        combined = torch.cat((cnn_out, vit_out), dim=1)\n",
        "        x = torch.relu(self.fc1(combined))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Load Model\n",
        "def load_model(model_path, num_classes=11):\n",
        "    model = CustomHybridModel(num_classes)\n",
        "    state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n",
        "    model.load_state_dict(state_dict, strict=False)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "# Label Mapping\n",
        "label_mapping = {\n",
        "    0: \"Backpack\",\n",
        "    1: \"Badminton Racquet\",\n",
        "    2: \"Kids' Watch\",\n",
        "    3: \"Refrigerator\",\n",
        "    4: \"Shirt\",\n",
        "    5: \"Speaker\",\n",
        "    6: \"Suitcase\",\n",
        "    7: \"Sunglasses\",\n",
        "    8: \"Television\",\n",
        "    9: \"Washing Machine\",\n",
        "    10: \"Men's Sports Shoe\"\n",
        "}\n",
        "\n",
        "# Color Extraction\n",
        "def extract_colors(image, n_colors=3):\n",
        "    img_np = np.array(image)\n",
        "    img_reshaped = img_np.reshape((-1, 3))\n",
        "    kmeans = KMeans(n_clusters=n_colors).fit(img_reshaped)\n",
        "    colors = kmeans.cluster_centers_.astype(int)\n",
        "\n",
        "    color_names = []\n",
        "    for color in colors:\n",
        "        r, g, b = color\n",
        "        if r > 200 and g > 200 and b > 200:\n",
        "            color_names.append(\"white\")\n",
        "        elif r > 200 and g < 100 and b < 100:\n",
        "            color_names.append(\"red\")\n",
        "        elif r < 100 and g > 200 and b < 100:\n",
        "            color_names.append(\"green\")\n",
        "        elif r < 100 and g < 100 and b > 200:\n",
        "            color_names.append(\"blue\")\n",
        "        elif r > 200 and g > 200 and b < 100:\n",
        "            color_names.append(\"yellow\")\n",
        "        elif r > 150 and g > 100 and b < 100:\n",
        "            color_names.append(\"orange\")\n",
        "        elif r < 50 and g < 50 and b < 50:\n",
        "            color_names.append(\"black\")\n",
        "        else:\n",
        "            color_names.append(\"gray\")\n",
        "\n",
        "    return list(set(color_names))\n",
        "\n",
        "# Shape Detection\n",
        "def detect_shapes(image):\n",
        "    gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)\n",
        "    edges = cv2.Canny(gray, 30, 150)\n",
        "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    shapes = []\n",
        "    for cnt in contours:\n",
        "        approx = cv2.approxPolyDP(cnt, 0.04 * cv2.arcLength(cnt, True), True)\n",
        "        if len(approx) == 3:\n",
        "            shapes.append(\"triangular\")\n",
        "        elif len(approx) == 4:\n",
        "            shapes.append(\"rectangular\")\n",
        "        elif len(approx) > 10:\n",
        "            shapes.append(\"circular\")\n",
        "        else:\n",
        "            shapes.append(\"irregular\")\n",
        "\n",
        "    return list(set(shapes))\n",
        "\n",
        "# Brand Extraction\n",
        "def extract_brand(image):\n",
        "    reader = easyocr.Reader(['en'])\n",
        "    ocr_results = reader.readtext(np.array(image), detail=0)\n",
        "\n",
        "    if ocr_results:\n",
        "        filtered_texts = [text for text in ocr_results if text.isalnum() and len(text) > 2]\n",
        "        if filtered_texts:\n",
        "            brand = max(set(filtered_texts), key=filtered_texts.count)\n",
        "            return brand.capitalize()\n",
        "    return \"Unbranded\"\n",
        "\n",
        "# Description Generation\n",
        "def generate_description(label, colors, shapes, brand):\n",
        "    color_desc = ', '.join(colors)\n",
        "    shape_desc = ', '.join(shapes)\n",
        "    accessory = \"\"\n",
        "\n",
        "    if \"Backpack\" in label:\n",
        "        accessory = \"with multiple compartments and durable straps\"\n",
        "    elif \"Suitcase\" in label:\n",
        "        accessory = \"featuring a telescopic handle and secure locks\"\n",
        "    elif \"Watch\" in label:\n",
        "        accessory = \"with a sleek wrist strap and water resistance\"\n",
        "    elif \"Shirt\" in label:\n",
        "        accessory = \"made from comfortable, breathable fabric\"\n",
        "\n",
        "    description_templates = [\n",
        "        f\"This is a {label.lower()} with a {color_desc} color scheme and a {shape_desc} design, {accessory}.\",\n",
        "        f\"A stylish {label.lower()} in shades of {color_desc}, {shape_desc} in structure, {accessory}.\",\n",
        "        f\"This {label.lower()} stands out with its {color_desc} tones and {shape_desc} shape, ideal for practical use. Brand: {brand}.\",\n",
        "        f\"An eye-catching {label.lower()} with {color_desc} colors, a {shape_desc} style, and {accessory}.\"\n",
        "    ]\n",
        "\n",
        "    if brand != \"Unbranded\":\n",
        "        description_templates.append(f\"A {label.lower()} by {brand}, showcasing {color_desc} hues, a {shape_desc} shape, and {accessory}.\")\n",
        "\n",
        "    return random.choice(description_templates)\n",
        "\n",
        "# Main Analysis Pipeline\n",
        "def analyze_image(image_path, model, label_mapping):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    img_tensor = transform(image).unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(img_tensor)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        label = label_mapping[predicted.item()]\n",
        "\n",
        "    colors = extract_colors(image)\n",
        "    shapes = detect_shapes(image)\n",
        "    brand = extract_brand(image)\n",
        "    description = generate_description(label, colors, shapes, brand)\n",
        "\n",
        "    print(f\"Label: {label}\")\n",
        "    print(f\"Colors: {colors}\")\n",
        "    print(f\"Shapes: {shapes}\")\n",
        "    print(f\"Brand: {brand}\")\n",
        "    print(f\"Description: {description}\")\n",
        "\n",
        "# Run Analysis\n",
        "model_path = '/content/drive/MyDrive/Labelling/saved_models_2/best_model_epoch_2.pth'\n",
        "model = load_model(model_path)\n",
        "\n",
        "# Analyze Image\n",
        "analyze_image('/content/shirt.jpg', model, label_mapping)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIjm-AxwgkKw",
        "outputId": "de477363-5885-4bd9-8074-ef04716b6805"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.13.0\n",
            "<ipython-input-4-203cddf90c3b>:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n",
            "WARNING:easyocr.easyocr:Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: Kids' Watch\n",
            "Colors: ['gray', 'white']\n",
            "Shapes: ['triangular', 'rectangular', 'irregular']\n",
            "Brand: Nike\n",
            "Description: A stylish kids' watch in shades of gray, white, triangular, rectangular, irregular in structure, with a sleek wrist strap and water resistance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "import cv2\n",
        "import easyocr\n",
        "import random\n",
        "\n",
        "# Custom Hybrid Model Definition\n",
        "class CustomHybridModel(nn.Module):\n",
        "    def __init__(self, num_classes=11):\n",
        "        super(CustomHybridModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.vit = torch.hub.load('pytorch/vision:v0.13.0', 'vit_b_16', weights=\"IMAGENET1K_V1\")\n",
        "        self.vit.heads = nn.Identity()  # Removing classification head\n",
        "\n",
        "        # Adjusted for compatibility\n",
        "        self.fc1 = nn.Linear(32 * 56 * 56 + 768, 128)  # Changed to match saved model\n",
        "        self.fc2 = nn.Linear(128, num_classes)  # Changed to match saved model\n",
        "\n",
        "    def forward(self, x):\n",
        "        cnn_out = self.pool(torch.relu(self.conv1(x)))\n",
        "        cnn_out = self.pool(torch.relu(self.conv2(cnn_out)))\n",
        "        cnn_out = cnn_out.view(-1, 32 * 56 * 56)\n",
        "\n",
        "        vit_out = self.vit(x)\n",
        "        if vit_out.dim() == 2:\n",
        "            vit_out = vit_out[:, :768]\n",
        "\n",
        "        combined = torch.cat((cnn_out, vit_out), dim=1)\n",
        "        x = torch.relu(self.fc1(combined))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Load Model\n",
        "def load_model(model_path, num_classes=11):\n",
        "    model = CustomHybridModel(num_classes)\n",
        "    state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n",
        "    model.load_state_dict(state_dict, strict=False)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "# Label Mapping\n",
        "label_mapping = {\n",
        "    0: \"Backpack\",\n",
        "    1: \"Badminton Racquet\",\n",
        "    2: \"Kids' Watch\",\n",
        "    3: \"Refrigerator\",\n",
        "    4: \"Shirt\",\n",
        "    5: \"Speaker\",\n",
        "    6: \"Suitcase\",\n",
        "    7: \"Sunglasses\",\n",
        "    8: \"Television\",\n",
        "    9: \"Washing Machine\",\n",
        "    10: \"Men's Sports Shoe\"\n",
        "}\n",
        "\n",
        "# Color Extraction\n",
        "def extract_colors(image, n_colors=3):\n",
        "    img_np = np.array(image)\n",
        "    img_reshaped = img_np.reshape((-1, 3))\n",
        "    kmeans = KMeans(n_clusters=n_colors).fit(img_reshaped)\n",
        "    colors = kmeans.cluster_centers_.astype(int)\n",
        "\n",
        "    color_names = []\n",
        "    for color in colors:\n",
        "        r, g, b = color\n",
        "        if r > 200 and g > 200 and b > 200:\n",
        "            color_names.append(\"white\")\n",
        "        elif r > 200 and g < 100 and b < 100:\n",
        "            color_names.append(\"red\")\n",
        "        elif r < 100 and g > 200 and b < 100:\n",
        "            color_names.append(\"green\")\n",
        "        elif r < 100 and g < 100 and b > 200:\n",
        "            color_names.append(\"blue\")\n",
        "        elif r > 200 and g > 200 and b < 100:\n",
        "            color_names.append(\"yellow\")\n",
        "        elif r > 150 and g > 100 and b < 100:\n",
        "            color_names.append(\"orange\")\n",
        "        elif r < 50 and g < 50 and b < 50:\n",
        "            color_names.append(\"black\")\n",
        "        else:\n",
        "            color_names.append(\"gray\")\n",
        "\n",
        "    return list(set(color_names))\n",
        "\n",
        "# Shape Detection\n",
        "def detect_shapes(image):\n",
        "    gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)\n",
        "    edges = cv2.Canny(gray, 30, 150)\n",
        "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    shapes = []\n",
        "    for cnt in contours:\n",
        "        approx = cv2.approxPolyDP(cnt, 0.04 * cv2.arcLength(cnt, True), True)\n",
        "        if len(approx) == 3:\n",
        "            shapes.append(\"triangular\")\n",
        "        elif len(approx) == 4:\n",
        "            shapes.append(\"rectangular\")\n",
        "        elif len(approx) > 10:\n",
        "            shapes.append(\"circular\")\n",
        "        else:\n",
        "            shapes.append(\"irregular\")\n",
        "\n",
        "    return list(set(shapes))\n",
        "\n",
        "# Brand Extraction\n",
        "def extract_brand(image):\n",
        "    reader = easyocr.Reader(['en'])\n",
        "    ocr_results = reader.readtext(np.array(image), detail=0)\n",
        "\n",
        "    if ocr_results:\n",
        "        filtered_texts = [text for text in ocr_results if text.isalnum() and len(text) > 2]\n",
        "        if filtered_texts:\n",
        "            brand = max(set(filtered_texts), key=filtered_texts.count)\n",
        "            return brand.capitalize()\n",
        "    return \"Unbranded\"\n",
        "\n",
        "# Description Generation\n",
        "def generate_description(label, colors, shapes, brand):\n",
        "    color_desc = ', '.join(colors)\n",
        "    shape_desc = ', '.join(shapes)\n",
        "    accessory = \"\"\n",
        "\n",
        "    if \"Backpack\" in label:\n",
        "        accessory = \"with multiple compartments and durable straps\"\n",
        "    elif \"Suitcase\" in label:\n",
        "        accessory = \"featuring a telescopic handle and secure locks\"\n",
        "    elif \"Watch\" in label:\n",
        "        accessory = \"with a sleek wrist strap and water resistance\"\n",
        "    elif \"Shirt\" in label:\n",
        "        accessory = \"made from comfortable, breathable fabric\"\n",
        "\n",
        "    description_templates = [\n",
        "        f\"This is a {label.lower()} with a {color_desc} color scheme and a {shape_desc} design, {accessory}.\",\n",
        "        f\"A stylish {label.lower()} in shades of {color_desc}, {shape_desc} in structure, {accessory}.\",\n",
        "        f\"This {label.lower()} stands out with its {color_desc} tones and {shape_desc} shape, ideal for practical use. Brand: {brand}.\",\n",
        "        f\"An eye-catching {label.lower()} with {color_desc} colors, a {shape_desc} style, and {accessory}.\"\n",
        "    ]\n",
        "\n",
        "    if brand != \"Unbranded\":\n",
        "        description_templates.append(f\"A {label.lower()} by {brand}, showcasing {color_desc} hues, a {shape_desc} shape, and {accessory}.\")\n",
        "\n",
        "    return random.choice(description_templates)\n",
        "\n",
        "# Main Analysis Pipeline\n",
        "def analyze_image(image_path, model, label_mapping):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    img_tensor = transform(image).unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(img_tensor)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        label = label_mapping[predicted.item()]\n",
        "\n",
        "    colors = extract_colors(image)\n",
        "    shapes = detect_shapes(image)\n",
        "    brand = extract_brand(image)\n",
        "    description = generate_description(label, colors, shapes, brand)\n",
        "\n",
        "    print(f\"Label: {label}\")\n",
        "    print(f\"Colors: {colors}\")\n",
        "    print(f\"Shapes: {shapes}\")\n",
        "    print(f\"Brand: {brand}\")\n",
        "    print(f\"Description: {description}\")\n",
        "\n",
        "# Run Analysis\n",
        "model_path = '/content/drive/MyDrive/Labelling/saved_models_2/best_model_epoch_2.pth'\n",
        "model = load_model(model_path)\n",
        "\n",
        "# Analyze Image\n",
        "analyze_image('/content/fgg.jpg', model, label_mapping)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0CoBYwCoalG",
        "outputId": "505991a6-752e-42bb-f9c3-9454f243d02c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.13.0\n",
            "<ipython-input-5-ab9caabd34a2>:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n",
            "WARNING:easyocr.easyocr:Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: Men's Sports Shoe\n",
            "Colors: ['gray', 'white']\n",
            "Shapes: ['irregular']\n",
            "Brand: Unbranded\n",
            "Description: This men's sports shoe stands out with its gray, white tones and irregular shape, ideal for practical use. Brand: Unbranded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RkJOLjlJsmyl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}